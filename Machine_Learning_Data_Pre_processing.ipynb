{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Data Pre-processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyappan24/Applied-AI-Notebooks/blob/master/Machine_Learning_Data_Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WRvqqdZik8Ez",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# AMAZON FINE FOOD REVIEWS : POLARITY CLASSIFICATION USIGN K-NN"
      ]
    },
    {
      "metadata": {
        "id": "GJc9lBjek6Ua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
        "\n",
        "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n",
        "\n",
        "Number of reviews: 568,454<br>\n",
        "Number of users: 256,059<br>\n",
        "Number of products: 74,258<br>\n",
        "Timespan: Oct 1999 - Oct 2012<br>\n",
        "Number of Attributes/Columns in data: 10 \n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1. Id\n",
        "2. ProductId - unique identifier for the product\n",
        "3. UserId - unqiue identifier for the user\n",
        "4. ProfileName\n",
        "5. HelpfulnessNumerator - number of users who found the review helpful\n",
        "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
        "7. Score - rating between 1 and 5\n",
        "8. Time - timestamp for the review\n",
        "9. Summary - brief summary of the review\n",
        "10. Text - text of the review"
      ]
    },
    {
      "metadata": {
        "id": "bQwQli2Ck_EY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Installing Libraries and Dependencies in Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "KsiSvR_ck-ty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "747e0ffd-2177-41a7-e3ad-6ff5166e85ab"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 702kB/s \n",
            "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 13.5MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/b9/7acc466df1f41a8c1f0a74e371ec7ee627162d325b80d7201dfd7b9521b1/boto3-1.9.35-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting botocore<1.13.0,>=1.12.35 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/76/5674019dcdc6475363a1b6fe93ec6dfdce05ec2e94339b177a1ac782d8f6/botocore-1.12.35-py2.py3-none-any.whl (4.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.7MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.35->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.35->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 13.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.35 botocore-1.12.35 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-K9m1h9qk-cW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f104c49b-de77-45bf-b983-6ff628c921b5"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "HnzPDg9djrY3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns \n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from tqdm import tqdm\n",
        "import sqlite3\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnHYBcBzkh3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "##Mounting google drive to get the data from the google drive folder"
      ]
    },
    {
      "metadata": {
        "id": "xoeuoljbkhOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1bcb676-0890-422e-a4d1-b04a3a76ec24"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G6LdD7phk2G4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the data\n",
        "\n",
        "The dataset is available in two forms\n",
        "1. .csv file\n",
        "2. SQLite Database\n",
        "\n",
        "In order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently.\n",
        "<br> \n",
        "\n",
        "Here as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\"."
      ]
    },
    {
      "metadata": {
        "id": "a4y8F46nkBHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "con = sqlite3.connect('/content/gdrive/My Drive/AppliedAI/Amazon Fine Food Reviews/database.sqlite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQSei1VWkg1q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qkAgI87plSLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "4a2f74bd-3c2a-4778-bf2f-19bf1be7e326"
      },
      "cell_type": "code",
      "source": [
        "filtered_data.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId          UserId                      ProfileName  \\\n",
              "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
              "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
              "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     1                       1      5  1303862400   \n",
              "1                     0                       0      1  1346976000   \n",
              "2                     1                       1      4  1219017600   \n",
              "\n",
              "                 Summary                                               Text  \n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "FjZwnwS3lT0x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partition(x):\n",
        "    if x < 3:\n",
        "        return 0\n",
        "    return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YSGYUnQlYJy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "actualScore = filtered_data['Score']\n",
        "positiveNegative = actualScore.map(partition) \n",
        "filtered_data['Score'] = positiveNegative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LFuUzRdflac_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "4648a77b-fb92-42e3-e373-e3c71e9be22f"
      },
      "cell_type": "code",
      "source": [
        "filtered_data.head(3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId          UserId                      ProfileName  \\\n",
              "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
              "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
              "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     1                       1      1  1303862400   \n",
              "1                     0                       0      0  1346976000   \n",
              "2                     1                       1      1  1219017600   \n",
              "\n",
              "                 Summary                                               Text  \n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "zHhb9BSblbwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02e4d5d2-a27c-4731-8595-ccad0bec925d"
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of data points in our data\", filtered_data.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points in our data (525814, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hdhQkTw-lmpi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Removing the Duplicate entries ** "
      ]
    },
    {
      "metadata": {
        "id": "X_HBP6tGld3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "et0AJimPlhrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0daa429e-9446-401d-bc30-90b4a58b0763"
      },
      "cell_type": "code",
      "source": [
        "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
        "final.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364173, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "6YJ4-DCulu5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TEXT PRE-PROCESSING\n",
        "\n",
        "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
        "\n",
        "Hence in the Preprocessing phase we do the following in the order below:-\n",
        "\n",
        "1. Begin by removing the html tags\n",
        "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
        "3. Check if the word is made up of english letters and is not alpha-numeric\n",
        "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
        "5. Convert the word to lowercase\n",
        "6. Remove Stopwords\n",
        "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
        "\n",
        "After which we collect the words used to describe positive and negative reviews"
      ]
    },
    {
      "metadata": {
        "id": "7XKahwD-l0QS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** REMOVING HTML TAGS  and PUNCTUATIONS** "
      ]
    },
    {
      "metadata": {
        "id": "582TDDDnlj8n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "\n",
        "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLOeFIH0l12Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "sno = nltk.stem.SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLkAfi8Wl3gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#removing not and no from list of stopwords \n",
        "stop.remove('not')\n",
        "stop.remove('no')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9RcL2NqLl9Tq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86cee13e-0fb2-4fac-9c15-b6854ed97fe4"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('final.sqlite'):\n",
        "    i=0\n",
        "    str1=' '\n",
        "    final_string=[]\n",
        "    all_positive_words=[] # store words from +ve reviews here\n",
        "    all_negative_words=[] # store words from -ve reviews here.\n",
        "    s=''\n",
        "    for sent in tqdm(final['Text'].values):\n",
        "        filtered_sentence=[]\n",
        "        #print(sent);\n",
        "        sent=cleanhtml(sent) # remove HTMl tags\n",
        "        for w in sent.split():\n",
        "            for cleaned_words in cleanpunc(w).split():\n",
        "                if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
        "                    if(cleaned_words.lower() not in stop):\n",
        "                        s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
        "                        filtered_sentence.append(s)\n",
        "                        if (final['Score'].values)[i] == 'positive': \n",
        "                            all_positive_words.append(s) #list of all words used to describe positive reviews\n",
        "                        if(final['Score'].values)[i] == 'negative':\n",
        "                            all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
        "                    else:\n",
        "                        continue\n",
        "                else:\n",
        "                    continue \n",
        "        #print(filtered_sentence)\n",
        "        str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
        "        #print(\"***********************************************************************\")\n",
        "\n",
        "        final_string.append(str1)\n",
        "        i+=1\n",
        "        \n",
        "#############---- storing the data into .sqlite file ------########################\n",
        "    final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
        "    final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")\n",
        "        # store final table into an SQlLite table for future.\n",
        "    conn = sqlite3.connect('final.sqlite')\n",
        "    c=conn.cursor()\n",
        "    conn.text_factory = str\n",
        "    final.to_sql('Reviews', conn,  schema=None, if_exists='replace', \\\n",
        "                 index=True, index_label=None, chunksize=None, dtype=None)\n",
        "    conn.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 364173/364173 [09:09<00:00, 662.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "IgzYCLx8mC9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if os.path.isfile('final.sqlite'):\n",
        "    conn = sqlite3.connect('final.sqlite')\n",
        "    final = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", conn)\n",
        "    conn.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lYgJma-zpGsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "8a639f38-2d1c-4538-f685-aeb51638673e"
      },
      "cell_type": "code",
      "source": [
        "final.head(2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>CleanedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138706</td>\n",
              "      <td>150524</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>ACITT7DI6IDDL</td>\n",
              "      <td>shari zychinski</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>939340800</td>\n",
              "      <td>EVERY book is educational</td>\n",
              "      <td>this witty little book makes my son laugh at l...</td>\n",
              "      <td>witti littl book make son laugh loud recit car...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>138688</td>\n",
              "      <td>150506</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>A2IW4PEEKO2R0U</td>\n",
              "      <td>Tracy</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1194739200</td>\n",
              "      <td>Love the book, miss the hard cover version</td>\n",
              "      <td>I grew up reading these Sendak books, and watc...</td>\n",
              "      <td>grew read sendak book watch realli rosi movi i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index      Id   ProductId          UserId      ProfileName  \\\n",
              "0  138706  150524  0006641040   ACITT7DI6IDDL  shari zychinski   \n",
              "1  138688  150506  0006641040  A2IW4PEEKO2R0U            Tracy   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     0                       0      1   939340800   \n",
              "1                     1                       1      1  1194739200   \n",
              "\n",
              "                                      Summary  \\\n",
              "0                   EVERY book is educational   \n",
              "1  Love the book, miss the hard cover version   \n",
              "\n",
              "                                                Text  \\\n",
              "0  this witty little book makes my son laugh at l...   \n",
              "1  I grew up reading these Sendak books, and watc...   \n",
              "\n",
              "                                         CleanedText  \n",
              "0  witti littl book make son laugh loud recit car...  \n",
              "1  grew read sendak book watch realli rosi movi i...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "h1vG87TUpQRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final = final.sort_values(by ='Time',ascending= True).copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q62Ov7bBpNAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1588
        },
        "outputId": "205b4b37-3735-4fba-f767-0cc6f2c266ee"
      },
      "cell_type": "code",
      "source": [
        "final.head(15)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>CleanedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138706</td>\n",
              "      <td>150524</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>ACITT7DI6IDDL</td>\n",
              "      <td>shari zychinski</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>939340800</td>\n",
              "      <td>EVERY book is educational</td>\n",
              "      <td>this witty little book makes my son laugh at l...</td>\n",
              "      <td>witti littl book make son laugh loud recit car...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>138683</td>\n",
              "      <td>150501</td>\n",
              "      <td>0006641040</td>\n",
              "      <td>AJ46FKXOVC7NR</td>\n",
              "      <td>Nicholas A Mesiano</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>940809600</td>\n",
              "      <td>This whole series is great way to spend time w...</td>\n",
              "      <td>I can remember seeing the show when it aired o...</td>\n",
              "      <td>rememb see show air televis year ago child sis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>417839</td>\n",
              "      <td>451856</td>\n",
              "      <td>B00004CXX9</td>\n",
              "      <td>AIUWLEQ1ADEG5</td>\n",
              "      <td>Elizabeth Medina</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>944092800</td>\n",
              "      <td>Entertainingl Funny!</td>\n",
              "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
              "      <td>beetlejuic well written movi everyth excel act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>346055</td>\n",
              "      <td>374359</td>\n",
              "      <td>B00004CI84</td>\n",
              "      <td>A344SMIA5JECGM</td>\n",
              "      <td>Vincent P. Ross</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>944438400</td>\n",
              "      <td>A modern day fairy tale</td>\n",
              "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
              "      <td>twist rumplestiskin captur film star michael k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>417838</td>\n",
              "      <td>451855</td>\n",
              "      <td>B00004CXX9</td>\n",
              "      <td>AJH6LUC1UT1ON</td>\n",
              "      <td>The Phantom of the Opera</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>946857600</td>\n",
              "      <td>FANTASTIC!</td>\n",
              "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
              "      <td>beetlejuic excel funni movi keaton hilari wack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>346116</td>\n",
              "      <td>374422</td>\n",
              "      <td>B00004CI84</td>\n",
              "      <td>A1048CYU0OV4O8</td>\n",
              "      <td>Judy L. Eans</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>947376000</td>\n",
              "      <td>GREAT</td>\n",
              "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
              "      <td>one movi movi collect fill comedi action whate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>346041</td>\n",
              "      <td>374343</td>\n",
              "      <td>B00004CI84</td>\n",
              "      <td>A1B2IZU1JLZA6</td>\n",
              "      <td>Wes</td>\n",
              "      <td>19</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>948240000</td>\n",
              "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
              "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
              "      <td>alway enjoy movi funni entertain didnt hesit p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>70688</td>\n",
              "      <td>76882</td>\n",
              "      <td>B00002N8SM</td>\n",
              "      <td>A32DW342WBJ6BX</td>\n",
              "      <td>Buttersugar</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>948672000</td>\n",
              "      <td>A sure death for flies</td>\n",
              "      <td>I bought a few of these after my apartment was...</td>\n",
              "      <td>bought apart infest fruit fli hour trap mani f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>346141</td>\n",
              "      <td>374450</td>\n",
              "      <td>B00004CI84</td>\n",
              "      <td>ACJR7EQF9S6FP</td>\n",
              "      <td>Jeremy Robertson</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>951523200</td>\n",
              "      <td>Bettlejuice...Bettlejuice...BETTLEJUICE!</td>\n",
              "      <td>What happens when you say his name three times...</td>\n",
              "      <td>happen say name three time michael keaten star...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>346094</td>\n",
              "      <td>374400</td>\n",
              "      <td>B00004CI84</td>\n",
              "      <td>A2DEE7F9XKP3ZR</td>\n",
              "      <td>jerome</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>959990400</td>\n",
              "      <td>Research - Beatlejuice video - French version</td>\n",
              "      <td>I'm getting crazy.I'm looking for Beatlejuice ...</td>\n",
              "      <td>get crazi look beatlejuic french version video...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>417883</td>\n",
              "      <td>451903</td>\n",
              "      <td>B00004CXX9</td>\n",
              "      <td>A2DEE7F9XKP3ZR</td>\n",
              "      <td>jerome</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>959990400</td>\n",
              "      <td>Research</td>\n",
              "      <td>I'm getting crazy.&lt;p&gt;Is it really impossible t...</td>\n",
              "      <td>get crazi realli imposs today not find french ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>1146</td>\n",
              "      <td>1245</td>\n",
              "      <td>B00002Z754</td>\n",
              "      <td>A29Z5PI9BW2PU3</td>\n",
              "      <td>Robbie</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>961718400</td>\n",
              "      <td>Great Product</td>\n",
              "      <td>This was a really good idea and the final prod...</td>\n",
              "      <td>realli good idea final product outstand use de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>1145</td>\n",
              "      <td>1244</td>\n",
              "      <td>B00002Z754</td>\n",
              "      <td>A3B8RCEI0FXFI6</td>\n",
              "      <td>B G Chase</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>962236800</td>\n",
              "      <td>WOW Make your own 'slickers' !</td>\n",
              "      <td>I just received my shipment and could hardly w...</td>\n",
              "      <td>receiv shipment could hard wait tri product lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>121041</td>\n",
              "      <td>131217</td>\n",
              "      <td>B00004RAMX</td>\n",
              "      <td>A5NQLNC6QPGSI</td>\n",
              "      <td>Kim Nason</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>965001600</td>\n",
              "      <td>End your Gopher Problems</td>\n",
              "      <td>I have just recently purchased the Woodstream ...</td>\n",
              "      <td>recent purchas woodstream corp gopher trap wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>138017</td>\n",
              "      <td>149789</td>\n",
              "      <td>B00004S1C6</td>\n",
              "      <td>A1KXONFPU2XQ5K</td>\n",
              "      <td>Stephanie Manley</td>\n",
              "      <td>26</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>965779200</td>\n",
              "      <td>A must have!</td>\n",
              "      <td>These are easy to use, they do not make a mess...</td>\n",
              "      <td>easi use not make mess offer vibrant color not...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      index      Id   ProductId          UserId               ProfileName  \\\n",
              "0    138706  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
              "30   138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
              "424  417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
              "330  346055  374359  B00004CI84  A344SMIA5JECGM           Vincent P. Ross   \n",
              "423  417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
              "245  346116  374422  B00004CI84  A1048CYU0OV4O8              Judy L. Eans   \n",
              "308  346041  374343  B00004CI84   A1B2IZU1JLZA6                       Wes   \n",
              "215   70688   76882  B00002N8SM  A32DW342WBJ6BX               Buttersugar   \n",
              "261  346141  374450  B00004CI84   ACJR7EQF9S6FP          Jeremy Robertson   \n",
              "325  346094  374400  B00004CI84  A2DEE7F9XKP3ZR                    jerome   \n",
              "427  417883  451903  B00004CXX9  A2DEE7F9XKP3ZR                    jerome   \n",
              "241    1146    1245  B00002Z754  A29Z5PI9BW2PU3                    Robbie   \n",
              "242    1145    1244  B00002Z754  A3B8RCEI0FXFI6                 B G Chase   \n",
              "485  121041  131217  B00004RAMX   A5NQLNC6QPGSI                 Kim Nason   \n",
              "868  138017  149789  B00004S1C6  A1KXONFPU2XQ5K          Stephanie Manley   \n",
              "\n",
              "     HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
              "0                       0                       0      1  939340800   \n",
              "30                      2                       2      1  940809600   \n",
              "424                     0                       0      1  944092800   \n",
              "330                     1                       2      1  944438400   \n",
              "423                     0                       0      1  946857600   \n",
              "245                     2                       2      1  947376000   \n",
              "308                    19                      23      0  948240000   \n",
              "215                     0                       0      1  948672000   \n",
              "261                     2                       3      1  951523200   \n",
              "325                     0                       3      1  959990400   \n",
              "427                     0                       1      1  959990400   \n",
              "241                     7                       7      1  961718400   \n",
              "242                    10                      10      1  962236800   \n",
              "485                     7                       8      1  965001600   \n",
              "868                    26                      28      1  965779200   \n",
              "\n",
              "                                               Summary  \\\n",
              "0                            EVERY book is educational   \n",
              "30   This whole series is great way to spend time w...   \n",
              "424                               Entertainingl Funny!   \n",
              "330                            A modern day fairy tale   \n",
              "423                                         FANTASTIC!   \n",
              "245                                              GREAT   \n",
              "308    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
              "215                             A sure death for flies   \n",
              "261           Bettlejuice...Bettlejuice...BETTLEJUICE!   \n",
              "325      Research - Beatlejuice video - French version   \n",
              "427                                           Research   \n",
              "241                                      Great Product   \n",
              "242                     WOW Make your own 'slickers' !   \n",
              "485                           End your Gopher Problems   \n",
              "868                                       A must have!   \n",
              "\n",
              "                                                  Text  \\\n",
              "0    this witty little book makes my son laugh at l...   \n",
              "30   I can remember seeing the show when it aired o...   \n",
              "424  Beetlejuice is a well written movie ..... ever...   \n",
              "330  A twist of rumplestiskin captured on film, sta...   \n",
              "423  Beetlejuice is an excellent and funny movie. K...   \n",
              "245  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
              "308  I, myself always enjoyed this movie, it's very...   \n",
              "215  I bought a few of these after my apartment was...   \n",
              "261  What happens when you say his name three times...   \n",
              "325  I'm getting crazy.I'm looking for Beatlejuice ...   \n",
              "427  I'm getting crazy.<p>Is it really impossible t...   \n",
              "241  This was a really good idea and the final prod...   \n",
              "242  I just received my shipment and could hardly w...   \n",
              "485  I have just recently purchased the Woodstream ...   \n",
              "868  These are easy to use, they do not make a mess...   \n",
              "\n",
              "                                           CleanedText  \n",
              "0    witti littl book make son laugh loud recit car...  \n",
              "30   rememb see show air televis year ago child sis...  \n",
              "424  beetlejuic well written movi everyth excel act...  \n",
              "330  twist rumplestiskin captur film star michael k...  \n",
              "423  beetlejuic excel funni movi keaton hilari wack...  \n",
              "245  one movi movi collect fill comedi action whate...  \n",
              "308  alway enjoy movi funni entertain didnt hesit p...  \n",
              "215  bought apart infest fruit fli hour trap mani f...  \n",
              "261  happen say name three time michael keaten star...  \n",
              "325  get crazi look beatlejuic french version video...  \n",
              "427  get crazi realli imposs today not find french ...  \n",
              "241  realli good idea final product outstand use de...  \n",
              "242  receiv shipment could hard wait tri product lo...  \n",
              "485  recent purchas woodstream corp gopher trap wit...  \n",
              "868  easi use not make mess offer vibrant color not...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "PjDyVq5zttVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Function to return the time stamp in DD/MM/YY HH : MM : SS format from UNIX FORMAT **"
      ]
    },
    {
      "metadata": {
        "id": "HDgF1HnRtPL9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ret_time(col):\n",
        "  return datetime.utcfromtimestamp(col).strftime('%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6yRmagzuBye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final['Formated_time'] = final['Time'].apply(ret_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sfzJGrVuKhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "33cfc33c-3933-4fd9-ce9c-2d0116b1dcad"
      },
      "cell_type": "code",
      "source": [
        "print(min(final['Formated_time']))\n",
        "print(max(final['Formated_time']))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1999-10-08 00:00:00\n",
            "2012-10-26 00:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QQsNljc5v6X6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Function to return the year of the timestamp **"
      ]
    },
    {
      "metadata": {
        "id": "4AQlfypLvj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ret_year(col):\n",
        "  val = pd.to_datetime(col)\n",
        "  return val.year"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PmrwuqLTwOZv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final['Year'] = final['Formated_time'].apply(ret_year)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLdQlx5avKSi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** APPROACH:** <br>\n",
        "1. Split the data into 70 - 30 where Train will have 70k points and test will have 30k points for ** WORD2VEC and TFIDF WORD2VEC representation**\n",
        "2. For ** BOW and TFIDF** representation we are splitting the dataset into 60k for train, 20k for CV, 20k for Test for reducing the time in predicting sparse matrices.\n",
        "3. For each Featurisation perform Cross validation for different K and find the optimal K and report the test accuracy\n",
        "4.. Standarad Scale the data before training and download the interim files which are generated for future re-use"
      ]
    },
    {
      "metadata": {
        "id": "rb81bDH7YdGC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#getting the already pre-processed final.sqlite stored in gdrive for emergency purposes when google cloab crashes\n",
        "conn = sqlite3.connect('/content/gdrive/My Drive/AppliedAI/Amazon Fine Food Reviews/final.sqlite')\n",
        "final = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", conn)\n",
        "conn.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oS2Gy5JB8PoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#sorting the data according to the time in ascending order\n",
        "final = final.sort_values(by='Formated_time').copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1AdGb7j6UlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 70 - 30 split \n",
        "\n",
        "Taking 70k Datapoints for Training and 30k Datapoints for testing "
      ]
    },
    {
      "metadata": {
        "id": "1IZAiPEg6UCP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = final[0:70000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFIFJfr3xut7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = final[70001:100001]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPbdOuKG62KS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "671b70a2-e428-46e1-c7bd-f078be6cbfc2"
      },
      "cell_type": "code",
      "source": [
        "pd.unique(train['Year'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "SODZiMGA7L8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c9827da-f337-4043-82f6-620b70477f8b"
      },
      "cell_type": "code",
      "source": [
        "pd.unique(test['Year'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2009, 2010])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "pNrnyRCl9rhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "------------------\n",
        "\n",
        "The data span of train is almost 10 years  and test is 1.5 years. \n",
        "\n",
        "-----------\n",
        "\n",
        "## FEATURIZATION 1 : AVERAGE WORD2VEC "
      ]
    },
    {
      "metadata": {
        "id": "Ah2uEtfP8GDx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training the word2vec on the train data \n",
        "i =0 \n",
        "list_of_sent = []\n",
        "for sent in train['CleanedText'].values:\n",
        "    list_of_sent.append(sent.split())\n",
        "\n",
        "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASN3ZpXH-1P9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i =0 \n",
        "list_of_sent_train = []\n",
        "for sent in train['CleanedText'].values:\n",
        "    list_of_sent_train.append(sent.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dJNEQFxs_D0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i =0 \n",
        "list_of_sent_test = []\n",
        "for sent in test['CleanedText'].values:\n",
        "    list_of_sent_test.append(sent.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hdJXfhH_QYB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing the Train dataset : Avg Word2Vec per each sentence in the training dataset Output : X_train , Y_train( Polarity) : Corresponding"
      ]
    },
    {
      "metadata": {
        "id": "Iv-JDzLG_IEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3aa8cf6b-eaa5-4184-975c-499859cf15d1"
      },
      "cell_type": "code",
      "source": [
        "sent_vectors_train = []\n",
        "for sent in tqdm(list_of_sent_train): # for each review/sentence\n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        if word in w2v_model:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        sent_vec /= cnt_words\n",
        "    sent_vectors_train.append(sent_vec)\n",
        "\n",
        "c= []\n",
        "for i in range(0,50):\n",
        "    c.append(\"Vector \" + str(i+1))\n",
        "    \n",
        "X_train = pd.DataFrame(sent_vectors_train,columns=c)\n",
        "y_train = train['Score']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [01:17<00:00, 907.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hy3W1BnbAZhy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing the Test dataset : Avg Word2Vec per each sentence in the test dataset Output : X_test , Y_test( Polarity) : Corresponding"
      ]
    },
    {
      "metadata": {
        "id": "DcPTUwAg_c-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8df52f8f-3e9d-4102-fd60-3db1d6f5473e"
      },
      "cell_type": "code",
      "source": [
        "sent_vectors_test= []\n",
        "for sent in tqdm(list_of_sent_test): # for each review/sentence\n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        if word in w2v_model:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        sent_vec /= cnt_words\n",
        "    sent_vectors_test.append(sent_vec)\n",
        "\n",
        "c= []\n",
        "for i in range(0,50):\n",
        "    c.append(\"Vector \" + str(i+1))\n",
        "    \n",
        "X_test = pd.DataFrame(sent_vectors_test,columns=c)\n",
        "y_test = list(test['Score'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:27<00:00, 1071.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8tgOkypOWXIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FEATURIZATION 2 : TFIDF WORD2VEC"
      ]
    },
    {
      "metadata": {
        "id": "tMjoXzsTWW3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#traing the tfidfVectorizer only for train dataset\n",
        "model = TfidfVectorizer(lowercase=True,ngram_range=(1,1))\n",
        "tf_idf_matrix = model.fit_transform(train['CleanedText'].values)\n",
        "# we are converting a dictionary with word as a key, and the idf as a value\n",
        "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y9oDNBnuaUCA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_________________\n",
        "\n",
        "Preparing the TFIDF WORD2VEC  of Train data \n",
        "_________________\n"
      ]
    },
    {
      "metadata": {
        "id": "DXMpzx7IV61y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8314c63b-8154-4945-ade5-a655709f1021"
      },
      "cell_type": "code",
      "source": [
        "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
        "row=0;\n",
        "for sent in tqdm(list_of_sent_train): # for each review/sentence \n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
        "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        if word in w2v_model:\n",
        "            vec = w2v_model.wv[word]\n",
        "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
        "            # to reduce the computation we are \n",
        "            # dictionary[word] = idf value of word in whole courpus\n",
        "            # sent.count(word) = tf values of word in this review\n",
        "            tf_idf = dictionary[word]*sent.count(word)\n",
        "            sent_vec += (vec * tf_idf)\n",
        "            weight_sum += tf_idf\n",
        "    if weight_sum != 0:\n",
        "        sent_vec /= weight_sum\n",
        "    tfidf_sent_vectors.append(sent_vec)\n",
        "    row += 1\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "c= []\n",
        "for i in range(0,50):\n",
        "    c.append(\"Vector \" + str(i+1))\n",
        "train_tfw2v = pd.DataFrame(tfidf_sent_vectors,columns=c)\n",
        "train_tfw2v['Score']= list(train['Score'])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [00:45<00:00, 1533.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VXPcgGyfaESX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_________________\n",
        "\n",
        "Preparing the TFIDF WORD2VEC  of Test data \n",
        "_________________\n"
      ]
    },
    {
      "metadata": {
        "id": "PlSERSXLWOWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8340eb46-22fe-47d3-8e2a-3f11e098bd51"
      },
      "cell_type": "code",
      "source": [
        "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
        "row=0;\n",
        "for sent in tqdm(list_of_sent_test): # for each review/sentence \n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
        "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        if word in w2v_model:\n",
        "            vec = w2v_model.wv[word]\n",
        "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
        "            # to reduce the computation we are \n",
        "            # dictionary[word] = idf value of word in whole courpus\n",
        "            # sent.count(word) = tf values of word in this review\n",
        "            tf_idf = dictionary[word]*sent.count(word)\n",
        "            sent_vec += (vec * tf_idf)\n",
        "            weight_sum += tf_idf\n",
        "    if weight_sum != 0:\n",
        "        sent_vec /= weight_sum\n",
        "    tfidf_sent_vectors.append(sent_vec)\n",
        "    row += 1\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "c= []\n",
        "for i in range(0,50):\n",
        "    c.append(\"Vector \" + str(i+1))\n",
        "test_tfw2v = pd.DataFrame(tfidf_sent_vectors,columns=c)\n",
        "test_tfw2v['Score']= list(test['Score'])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:21<00:00, 1414.42it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MZHb3s2QI2Ul",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-------------\n",
        "\n",
        "# 60-20-20 SPLIT\n"
      ]
    },
    {
      "metadata": {
        "id": "42_YI7KDJtnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = final[0:60000]\n",
        "cv = final[60001:80001]\n",
        "test = final[80001:100001]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-dJMnNwUEVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = train['Score']\n",
        "y_cv = cv['Score']\n",
        "y_test = test['Score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdTGV-PxJtgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc989d86-96f7-4069-b69b-ec085a1580ac"
      },
      "cell_type": "code",
      "source": [
        "pd.unique(train['Year'])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "b0WQxLmlJtVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76b1e1d3-3a7f-4373-e9e5-288dc0267498"
      },
      "cell_type": "code",
      "source": [
        "pd.unique(cv['Year'])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2009, 2010])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "1szGnG1cLFvU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27947cd0-0f58-48c4-97cb-9a68c01afd21"
      },
      "cell_type": "code",
      "source": [
        "pd.unique(test['Year'])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2010])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5nUKZbhHf5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FEATURIZATION 3 : BOW"
      ]
    },
    {
      "metadata": {
        "id": "3VlCuQ43Ey-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#intiailising the bag words count vectorizer\n",
        "count_vect = CountVectorizer() #in scikit-learn\n",
        "train_bow = count_vect.fit_transform(train['CleanedText'].values)\n",
        "test_bow = count_vect.transform(test['CleanedText'].values)\n",
        "cv_bow = count_vect.transform(cv['CleanedText'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zackCd_WVR9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FEATURIZATION 4 : TFIDF : UNIGRAM TF_IDF\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kpNYRmc2IocZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_idf = TfidfVectorizer(lowercase=True,ngram_range=(1,1))\n",
        "train_tfidf = tf_idf.fit_transform(train['CleanedText'].values)\n",
        "test_tfidf = tf_idf.transform(test['CleanedText'].values)\n",
        "cv_tfidf = tf_idf.transform(cv['CleanedText'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjal-_WfPP1K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pickle.dump(y_cv,open('y_cv.sav','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YyiKR2iMSUzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('y_test.sav') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UZCnajwfQ-SQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5f5a9338-9e64-49bf-f492-8af827459e6f"
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 111860\n",
            "-rw-r--r-- 1 root root 12241858 Nov  1 07:34 avg_word2vec_test.sav\n",
            "-rw-r--r-- 1 root root 28561864 Nov  1 07:22 avg_word2vec_train.sav\n",
            "-rw-r--r-- 1 root root  7608347 Nov  1 12:33 cv_bow.sav\n",
            "-rw-r--r-- 1 root root  7608320 Nov  1 12:36 cv_tfidf.sav\n",
            "drwx------ 3 root root     4096 Nov  1 06:13 gdrive\n",
            "drwxr-xr-x 2 root root     4096 Oct 30 22:07 sample_data\n",
            "-rw-r--r-- 1 root root  7439027 Nov  1 12:33 test_bow.sav\n",
            "-rw-r--r-- 1 root root  7439000 Nov  1 12:36 test_tfidf.sav\n",
            "-rw-r--r-- 1 root root 21808296 Nov  1 12:33 train_bow.sav\n",
            "-rw-r--r-- 1 root root 21808296 Nov  1 12:44 train_tfidf.sav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IVd3R800gjm7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "----------------\n",
        "<br>\n",
        "\n",
        "### ALL THE DATA OF TRAIN AND TEST OF RESPECTIVE FEATURIZATIONS HAS BEEN PUSHED INTO GOOGLE DRIVE IN PICKLE SERIALIZED FORMAT FOR DIRECT FUTURE USE"
      ]
    }
  ]
}